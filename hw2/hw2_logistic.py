# -*- coding: utf-8 -*-
"""ML_HW2_logistic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ntJXNyXVYuChXGCbTwYS4UuI7L34TegP

## ML HW2 手把手教學 

### Logistic regression
"""

import numpy as np
import pandas as pd

"""We only use one-hot-encoding feature here"""

def load_data():
    x_train = pd.read_csv('X_train')
    x_test = pd.read_csv('X_test')

    x_train = x_train.values
    x_test = x_test.values

    y_train = pd.read_csv('Y_train', header = None)
    y_train = y_train.values
    y_train = y_train.reshape(-1)

    return x_train, y_train, x_test

"""Use np.clip to prevent overflow"""

def sigmoid(z):
    res = 1 / (1.0 + np.exp(-z))
    return np.clip(res, 1e-6, 1-1e-6)

"""Feature normalize, only on continues variable"""

def normalize(x_train, x_test):
    
    x_all = np.concatenate((x_train, x_test), axis = 0)
    mean = np.mean(x_all, axis = 0)
    std = np.std(x_all, axis = 0)

    index = [0, 1, 3, 4, 5]
    mean_vec = np.zeros(x_all.shape[1])
    std_vec = np.ones(x_all.shape[1])
    mean_vec[index] = mean[index]
    std_vec[index] = std[index]

    x_all_nor = (x_all - mean_vec) / std_vec

    x_train_nor = x_all_nor[0:x_train.shape[0]]
    x_test_nor = x_all_nor[x_train.shape[0]:]

    return x_train_nor, x_test_nor

"""Gradient descent using adagrad"""

def train(x_train, y_train):
    w = np.zeros(x_train.shape[1])
    b = 0.0

    epoch = 1000
    lr = 0.01
    b_lr = 0
    w_lr = np.ones(x_train.shape[1])
    
    for e in range(epoch):
        z = np.dot(x_train , w) + b
        pred = sigmoid(z)
        loss = y_train - pred

		# Calculate gradient.
        w_grad = -1 * np.dot(loss , x_train)
        b_grad = -1 * np.sum(loss)

        # Update w and b.
        w_lr += w_grad**2
        b_lr += b_grad**2
        b -= lr / np.sqrt(b_lr) * b_grad
        w -= lr / np.sqrt(w_lr) * w_grad
        
        # Calculate loss and accuracy.
        loss = -1 * np.mean(y_train * np.log(pred + 1e-100) + (1 - y_train) * np.log(1 - pred + 1e-100))
        train_accuracy = accuracy(x_train , y_train , w , b)
        # validation_accuracy = accuracy(validation_x , validation_y , w , b)
        #print ("train accuracy: ", train_accuracy, "validation accuracy:" ,validation_accuracy)
        print ("num: ", e, ", train accuracy: ", train_accuracy)
    return w, b

def accuracy(x , y , weight , bias):
	count = 0
	number_of_data = x.shape[0]
	for i in range(number_of_data):
		probability = sigmoid(weight @ x[i] + bias)
		if ((probability > 0.5 and y[i] == 1) or (probability < 0.5 and y[i] == 0)):
			count += 1
	return count / number_of_data

def main():
    x_train, y_train, x_test = load_data()
    x_train, x_test = normalize(x_train, x_test)
    w, b = train(x_train, y_train)
    predict(x_test, w, b)

# TODO: predict x_test
def predict(x_test, w, b):
    y_test = list()
    number_of_data = x_test.shape[0]
    for i in range(number_of_data):
        y_test.append(1 if (sigmoid(w @ x_test[i] + b) > 0.5) else 0)
    import csv
    with open('predict.csv', 'w', newline='') as csvf:
        # 建立 CSV 檔寫入器
        writer = csv.writer(csvf)
        writer.writerow(['id','label'])
        for i in range(len(y_test)):
            writer.writerow( [i + 1, int(y_test[i])] )

if (__name__ == '__main__'):
	main()